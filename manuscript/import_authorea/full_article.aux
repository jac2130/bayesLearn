\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\bibstyle{plain}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Replace this text with your caption}}{1}{figure.1}}
\newlabel{fig:fully_rational}{{1}{1}{Replace this text with your caption\relax }{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces }}{2}{figure.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Distribution of step-by-step performance}{2}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  caption see text above}}{3}{figure.3}}
\newlabel{fig:convergence_humans}{{3}{3}{caption see text above\relax }{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}A metric to measure Exploration and Expoitation}{3}{section.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Binned (100 bins) density function of distance between 2 steps. This distance can be positive, which is equivalent to adding distance compared to the true model (in red). On the contrary, a negative distance shows improvement (in green). Both sides are heavy-tailed distributions, with very good fits with a power law model (see both insets). However, the exponents are different for improvement (exponent = 2.25), compared to counter performance (exponent, 1.34). This discrepancy explains why on average participants make improvements at a pace of $\alpha = 0.09 = 2.25 - 1.34$ {\bf  [mistake here 0.09 $\rightarrow $ 0.9 ! formal demonstration needed here $\rightarrow $ it implies a stochastic process with stochastic increment given this distribution, which is equivalent (in my mind) to considering a balanced stochastic component with exponent 1.34 on both sides, and a deterministic drift of 0.09]}. The values of the exponents tell us that progress (exponent = 2.25) has its first moment (mean) well defined, but variance diverges. On the contrary, counter performance (exponent = 1.34) has its mean and variance diverge as $n \rightarrow \infty $ (up to some boundaries, which we can see on both side in both insets $\rightarrow $ actually we could fit a power-law with a exponential crossover in the tail, or even better, impose a hard limit which must be 1 on both sides (in theory), i.e. at best (resp. worst) the participant can jump from guessing the worst possible model to the true model (from 1 to 0 $\rightarrow $ -1), (resp. having reached the best model, jump back to the worst (from 0 to 1 $\rightarrow $ 1). The point of maximum density is very close to but smaller than 0 with value -0.01. This 1\% improvement occurs 96\% of participant attempts. {\bf  [One may also want to consider the adjustment constants to quantify exactly how much it likely to make a jump of $|\epsilon |$ (either better performance or worse performace)]}.}}{4}{figure.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Relation between Perfect and Human Bayesian Learning}{5}{section.3}}
\newlabel{BHL}{{1}{5}{Relation between Perfect and Human Bayesian Learning\relax }{equation.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Propositions for explaining the difference between BL and HL}{5}{section.4}}
\citation{Hawkes_1974}
\citation{Crane_2008}
\citation{Maillart_2011}
\newlabel{eq:Hawkes}{{4}{6}{Rationale for a Hawkes Conditional Poisson Procees\relax }{equation.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Rational (Bayesian or Frequentist) Learning}{7}{section.5}}
\citation{March_1991}
\newlabel{tab:table}{{7}{8}{Some ideas for a model\relax }{Item.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}What we can observe from Matrices}{8}{section.6}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Some ideas for a model}{8}{section.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Triangular matrices of difference between {\bf  A} latest solution as a function of time and all former solutions, and {\bf  B} of mean distance between the true model and resp. $i$ and $j$ configurations. Matrix {\bf  B} shows the performance of a proposed model and serves as a reference for rationalizing choice sequences made by participants. These choice sequences are represented in matrix {\bf  A} for participant 13: We see a wealth of characteristic choices leading to better of worse solutions, but also some integration and disruptive change in strategies, some which having a positive impact on performance, and other having a negative impact on performance. The rectangle structures show that participants do not drastically update drastically, but rather alternate periods of fine tuning and radical innovation. Matrix {\bf  A} can also be watched in a more heuristic way at the coarse grained level: The more contrasted the pattern the more innovation overall. And, as shown here for participant 13, as more models are tested -- towards the lower right corner -- colors get more yellowish showing overall convergence of models. In the case presented here, the convergence of models leads overall to better solutions over time. It may not always be the case.}}{9}{figure.5}}
\newlabel{fig:matrices}{{5}{9}{Triangular matrices of difference between {\bf A} latest solution as a function of time and all former solutions, and {\bf B} of mean distance between the true model and resp. $i$ and $j$ configurations. Matrix {\bf B} shows the performance of a proposed model and serves as a reference for rationalizing choice sequences made by participants. These choice sequences are represented in matrix {\bf A} for participant 13: We see a wealth of characteristic choices leading to better of worse solutions, but also some integration and disruptive change in strategies, some which having a positive impact on performance, and other having a negative impact on performance. The rectangle structures show that participants do not drastically update drastically, but rather alternate periods of fine tuning and radical innovation. Matrix {\bf A} can also be watched in a more heuristic way at the coarse grained level: The more contrasted the pattern the more innovation overall. And, as shown here for participant 13, as more models are tested -- towards the lower right corner -- colors get more yellowish showing overall convergence of models. In the case presented here, the convergence of models leads overall to better solutions over time. It may not always be the case}{figure.5}{}}
\bibdata{bibliography/converted_to_latex.bib}
\@writefile{toc}{\contentsline {section}{\numberline {8}Exploitation versus Exploration}{10}{section.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Shall we consider the 8 dimensions together or indepedent?}{10}{subsection.8.1}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Next Steps}{10}{section.9}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Side ideas}{10}{section.10}}
