Binned (100 bins) density function of distance between 2 steps. This distance can be positive, which is equivalent to adding distance compared to the true model (in red). On the contrary, a negative distance shows improvement (in green). Both sides are heavy-tailed distributions, with very good fits with a power law model (see both insets). However, the exponents are different for improvement (exponent = 2.25), compared to counter performance (exponent, 1.34). This discrepancy explains why on average participants make improvements at a pace of $\alpha = 0.09 = 2.25 - 1.34$ {\bf [mistake here 0.09 $\rightarrow$ 0.9 ! formal demonstration needed here $\rightarrow$ it implies a stochastic process with stochastic increment given this distribution, which is equivalent (in my mind) to considering a balanced stochastic component with exponent 1.34 on both sides, and a deterministic drift of 0.09]}. The values of the exponents tell us that progress (exponent = 2.25) has its first moment (mean) well defined, but variance diverges. On the contrary, counter performance (exponent = 1.34) has its mean and variance diverge as $n \rightarrow \infty$ (up to some boundaries, which we can see on both side in both insets $\rightarrow$ actually we could fit a power-law with a exponential crossover in the tail, or even better, impose a hard limit which must be 1 on both sides (in theory), i.e. at best (resp. worst) the participant can jump from guessing the worst possible model to the true model (from 1 to 0 $\rightarrow$ -1), (resp. having reached the best model, jump back to the worst (from 0 to 1 $\rightarrow$ 1). The point of maximum density is very close to but smaller than 0 with value -0.01. This 1\% improvement occurs 96\% of participant attempts. {\bf [One may also want to consider the adjustment constants to quantify exactly how much it likely to make a jump of $|\epsilon|$ (either better performance or worse performace)]}.